{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplenn.optim.qlearning import ReplayMemory\n",
    "from simplenn.optim.qlearning import QLearning\n",
    "from simplenn.optim.qlearning import DoubleQLearning\n",
    "from simplenn.optim.qlearning import FixedTargetQLearning\n",
    "from simplenn.optim.qlearning import FixedTargetDoubleQLearningSymmetric\n",
    "from simplenn.optim.qlearning import FixedTargetDoubleQLearningAsymmetric\n",
    "from simplenn.optim.qlearning import BaseSimulation\n",
    "\n",
    "from simplenn.structures.qfunction import QTable\n",
    "from simplenn.structures.qfunction import DiscretizedQTable\n",
    "from simplenn.structures.qfunction import AutoDiscretizingQTable\n",
    "from simplenn.structures.qfunction import AggregateQFunction\n",
    "\n",
    "from simplenn.evaluation.simulation import Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('MountainCar-v0')\n",
    "#env = gym.make('Acrobot-v1')\n",
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config dict\n",
    "config = {}\n",
    "\n",
    "# Basic parameters\n",
    "config[\"gamma\"] = 0.95\n",
    "config[\"defaultVal\"] = 0.0\n",
    "\n",
    "# Learning rate related parameters\n",
    "config[\"alphaRate\"] = 0.75\n",
    "config[\"alphaCutoff\"] = 0.0\n",
    "\n",
    "# Exploration related parameters\n",
    "config[\"epsInit\"] = 1.0\n",
    "config[\"epsFinal\"] = 0.2\n",
    "config[\"epsRampLen\"] = 10\n",
    "\n",
    "# Auto-discretization related parameters\n",
    "config[\"maxObsPerLeaf\"] = 16\n",
    "config[\"maxLeafNodes\"] = 30*10**3\n",
    "config[\"splitResetMode\"] = AutoDiscretizingQTable.MODE_0\n",
    "\n",
    "# Experience replay related parameters\n",
    "config[\"replayMemMaxSize\"] = 100 * env.env.spec.max_episode_steps\n",
    "config[\"replayMemMinSize\"] = 100 * env.env.spec.max_episode_steps\n",
    "config[\"replayMemMode\"] = ReplayMemory.MODE_CYCLING\n",
    "\n",
    "# Fixed Q-target related parameters\n",
    "config[\"targetQUpdateFreq\"] = 25 * env.env.spec.max_episode_steps\n",
    "\n",
    "# Simulation duration\n",
    "config[\"nEpisodes\"] = 50000\n",
    "\n",
    "# Evaluation related parameters\n",
    "config[\"nTest\"] = 250\n",
    "config[\"verboseFreq\"] = 5000\n",
    "config[\"recordFreq\"] = 5000\n",
    "\n",
    "# Wandb only parameters\n",
    "config[\"envId\"] = env.env.spec.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCartPos = 50\n",
    "nCartV = 100\n",
    "nPoleAngle = 100\n",
    "nPoleV = 100\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "artMaxCartV = 5.0\n",
    "artMinCartV = -artMaxCartV\n",
    "artMaxPoleV = 10.0\n",
    "artMinPoleV = -artMaxPoleV\n",
    "\n",
    "qCartPos = (obs_space.high[0] - obs_space.low[0]) / nCartPos\n",
    "qCartV = (artMaxCartV - artMinCartV) / nCartV\n",
    "qPoleAngle = (obs_space.high[2] - obs_space.low[2]) / nPoleAngle\n",
    "qPoleV = (artMaxPoleV - artMinPoleV) / nPoleV\n",
    "\n",
    "config[\"quantums\"] = [qCartPos, qCartV, qPoleAngle, qPoleV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doLearning(config):\n",
    "    \n",
    "    # Components\n",
    "    \n",
    "#    Q = AutoDiscretizingQTable(\n",
    "#        range(env.action_space.n),\n",
    "#        config[\"gamma\"],\n",
    "#        config[\"alphaRate\"],\n",
    "#        config[\"alphaCutoff\"],\n",
    "#        config[\"epsInit\"],\n",
    "#        config[\"epsFinal\"],\n",
    "#        config[\"epsRampLen\"],\n",
    "#        config[\"defaultVal\"],\n",
    "#        config[\"maxObsPerLeaf\"],\n",
    "#        config[\"maxLeafNodes\"],\n",
    "#        config[\"splitResetMode\"]\n",
    "#    )\n",
    "    Q = DiscretizedQTable(\n",
    "        range(env.action_space.n),\n",
    "        config[\"gamma\"],\n",
    "        config[\"alphaRate\"],\n",
    "        config[\"alphaCutoff\"],\n",
    "        config[\"epsInit\"],\n",
    "        config[\"epsFinal\"],\n",
    "        config[\"epsRampLen\"],\n",
    "        config[\"defaultVal\"],\n",
    "        config[\"quantums\"]\n",
    "    )\n",
    "    config[\"qfunctionClass\"] = Q.__class__.__name__\n",
    "    \n",
    "    replayMem = ReplayMemory(\n",
    "        config[\"replayMemMaxSize\"], \n",
    "        config[\"replayMemMinSize\"], \n",
    "        config[\"replayMemMode\"]\n",
    "    )\n",
    "    \n",
    "#    algo = QLearning(Q, replayMem)\n",
    "    algo = DoubleQLearning(Q, replayMem)\n",
    "#    algo = FixedTargetQLearning(Q, replayMem, config[\"targetQUpdateFreq\"])\n",
    "#    algo = FixedTargetDoubleQLearningSymmetric(Q, replayMem, config[\"targetQUpdateFreq\"])\n",
    "#    algo = FixedTargetDoubleQLearningAsymmetric(Q, replayMem, config[\"targetQUpdateFreq\"])\n",
    "    config[\"algoClass\"] = algo.__class__.__name__\n",
    "    \n",
    "    sim = Simulation(env, config[\"nTest\"], True)\n",
    "    \n",
    "    # Running\n",
    "    \n",
    "    wandb.init(project=\"simple_rl\", config=config)\n",
    "    Q = algo.learn(\n",
    "        sim, \n",
    "        config[\"nEpisodes\"], \n",
    "        config[\"verboseFreq\"], \n",
    "        config[\"recordFreq\"], \n",
    "        wandb\n",
    "    )\n",
    "    \n",
    "    # Returning result\n",
    "    \n",
    "    return (Q, algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nRuns = 3\n",
    "\n",
    "for i in range(nRuns):\n",
    "    \n",
    "    print(f\"{i+1}/{nRuns}\")\n",
    "    Q, algo = doLearning(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Simulation(env, config[\"nTest\"], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 191.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score: {sim.performEpisode(Q, render=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    250.000000\n",
       "mean     153.092000\n",
       "std       37.480645\n",
       "min       67.000000\n",
       "25%      128.250000\n",
       "50%      149.000000\n",
       "75%      173.000000\n",
       "max      271.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sim.performEpisodes(Q)[1]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score and nStates evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f44adc5cfd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getActualQ(Q):\n",
    "    return Q if not isinstance(Q, AggregateQFunction) else Q.Qs[0]\n",
    "\n",
    "scores = [sim.performEpisodes(q)[0] for q in algo.qs]\n",
    "nStates = [len(getActualQ(q).data) for q in algo.qs]\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    \"Score\":pd.Series(scores),\n",
    "    \"nStates\":pd.Series(nStates)\n",
    "})\n",
    "\n",
    "ax = df.plot(y=\"Score\")\n",
    "df.plot(y=\"nStates\", secondary_y=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Values distribution (Simple Q-Learning only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f44adc359d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = []\n",
    "\n",
    "for s, sData in getActualQ(algo.Q).data.items():\n",
    "    qs += [aData[Q.Q_IDX] for a, aData in sData.items()]\n",
    "\n",
    "pd.Series(qs).hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions visit count distribution (Simple Q-Learning only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f44ad717250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = []\n",
    "\n",
    "for s, sData in getActualQ(algo.Q).data.items():\n",
    "    ns += [aData[Q.N_IDX] for a, aData in sData.items()]\n",
    "\n",
    "pd.Series(ns).hist(bins=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrepancies between Qa and Qb (Double Q-Learning only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f44accdcd90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not work on auto-dicretized QTables.\n",
    "\n",
    "actions = set()\n",
    "\n",
    "Qa = algo.qs[-1].Qs[0]\n",
    "Qb = algo.qs[-1].Qs[1]\n",
    "\n",
    "for s, sData in Qa.data.items():\n",
    "    for a, aData in sData.items():\n",
    "        actions.add((s, a))\n",
    "\n",
    "for s, sData in Qb.data.items():\n",
    "    for a, aData in sData.items():\n",
    "        actions.add((s, a))\n",
    "\n",
    "nDiff = []\n",
    "qDiff = []\n",
    "\n",
    "for s, a in actions:\n",
    "    \n",
    "    na = Qa.getData(s, a).get(Qa.N_IDX, 0)\n",
    "    nb = Qb.getData(s, a).get(Qb.N_IDX, 0)\n",
    "    nMid = (na + nb) / 2.0\n",
    "    \n",
    "    qa = Qa.getData(s, a).get(Qa.Q_IDX, 0.0)\n",
    "    qb = Qb.getData(s, a).get(Qb.Q_IDX, 0.0)\n",
    "    qMid = (qa + qb) / 2.0\n",
    "    \n",
    "    if na > 5 and nb > 5:\n",
    "        nDiff.append(abs(nb - nMid) / nMid)\n",
    "    if na > 0 and nb > 0:\n",
    "        qDiff.append(abs(qb - qMid) / qMid)\n",
    "\n",
    "pd.Series(nDiff).hist(bins=100, density=True, cumulative=True)\n",
    "#pd.Series(qDiff).hist(bins=100, density=True, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administrative tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "runs = api.runs(\"jrmsayag/simple_rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    run.config[\"alphaCutoff\"] = 0.0\n",
    "    run.config[\"algoClass\"] = QLearning.__name__\n",
    "    run.config[\"qfunctionClass\"] = DiscretizedQTable.__name__\n",
    "    run.tags.clear()\n",
    "    run.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
